%×××××××××××××××××××××××××××××××
%第3章内容
%×××××××××××××××××××××××××××××××

\chapter{语义感知的动态关键点剔除和描述增强}\xiaosi

\section{引言}
关键点提取与描述是图像配准\cite{rf1,rf2} 、视觉里程计\cite{rf3,rf4}（Visual Odometry, VO）、同时定位于地图重建\cite{rf5,rf6,rf7}（simultaneous localization and mapping，SLAM）与三维重建\cite{rf8}（Structure-from-Motion, SfM）的基础任务，其核心在于在不同视角或光照条件下准确地检测可重复关键点并生成判别性强的描述符。传统方法SIFT\cite{rf9}、ORB\cite{rf10} 通过局部光度变化检测关键点，并以手工设计的图像表征构建描述符。然而，这类方法仅关注关键点局部区域的可重复性，忽略了关键点的全局可靠性。

随着深度学习的发展，SuperPoint\cite{rf11}、ALike\cite{rf12} 等方法利用卷积神经网络的强大特征提取能力，在光照与视角变化下显著提升了关键点精准度和描述符的可重复性。进一步地，R2D2\cite{rf13}、XFeat\cite{rf14} 等方法通过引入全局一致性约束，分析整幅图像的描述符相似性以识别纹理重复区域，从而剔除低区分度的关键点并增强匹配鲁棒性。然而，这些方法仍未考虑动态场景中来自行人或车辆等对象的不可靠关键点，这些关键点在位姿估计中往往引入显著误差。

针对这一问题，部分研究SSP\cite{rf15}、SFD2\cite{rf16} 以及Cadar 等人\cite{rf17}尝试在描述符提取阶段引入语义特征联合训练，使描述符具备一定的语义判别能力。然而，这类方法仅在描述符层面融合语义信息，未能显式剔除动态物体等不可靠区域的关键点。同时，其语义与纹理特征融合方式相对简单，缺乏根据场景内容动态调整特征权重的能力。另一方面，部分语义SLAM方法\cite{rf18,rf19,rf20,rf26,bescos2021dynaslam,yu2018ds}引入独立的语义分割网络，以显式剔除车辆、行人等动态物体上的关键点。尽管这种方式能够利用语义信息过滤不可靠区域，但需要对同一输入图像执行两次神经网络推理，带来较高的计算成本。并且依赖精确的多类别语义标签辅助判断描述符的匹配，容易受到语义预测误差的影响，从而限制了其实用性与泛化能力。

本研究提出一种端到端的关键点检测与描述框架，将多语义识别任务简化为可靠与不可靠的二分类问题，并采用邻域差分算法剔除语义边界交接处的关键点，从而显著提升可靠性预测的准确性。在此基础上，本文设计了一种端到端的联合可靠性检测与描述网络，在同一特征提取过程中同时预测关键点关键点可靠性以及语义-几何-纹理融合描述符。具体而言，网络包含三个模块：关键点可重复性解码器（Repeatability Decoder）、可靠性解码器（Reliability Decoder）与描述符解码器（Descriptor Decoder）。其中如图3-1，可靠性解码器输出同分辨率的可靠性热力图。该热力图反映关键点在动态场景中的可置信度，并与可重复性热力图逐元素相乘，经过邻域差分算法剔除语义边界交接处的关键点，再经过非极大值抑制（NMS）后得到最终关键点分布。

同时，描述符解码器利用语义几何融合模块（Semantic-Geometric Fusion Module, SGFM），纹理描述符先和特征点的几何信息融合，然后再通过交叉注意力机制动态融合多尺度语义信息，使得网络能够根据场景内容自适应地平衡语义-几何-纹理特征的重要性，从而生成同时具备语义判别力与局部几何纹理判别性的特征描述符。

在训练阶段，本文进一步使用了语义排序损失\cite{rf28}函数（Semantic Ranking Loss）。该损失在描述符学习过程中引入语义相似性约束，使相同语义对象的描述符在特征空间中聚合，而不同语义对象的描述符分离。相比传统的三元组对比损失\cite{rf29,rf30}（Contrastive Loss），语义排序损失在保持局部纹理多样性的同时，显著提升了描述符的语义区分能力与匹配精度。

通过上述设计，本文的模型能够联合学习关键点的可重复性与可靠性，在无需显式语义分割的前提下自动剔除动态区域的不可靠关键点，并生成兼具语义感知与纹理细节的高判别性描述符，为后续视觉里程计与SLAM任务提供更稳定的特征匹配基础。本文的贡献如下：

(1)	将过去多类别语义分割简化为为关键点可靠性判断问题，提出端到端可靠性解码器，再经过邻域差分算法后与可重复性解码器结合生成最终更可靠的关键点分布。

(2)	本文设计了一种语义几何融合模块（SGFM），用特征点几何信息和可靠解码器中高层多分类语义特征对局部纹理描述符进行监督和融合，在复杂场景下显著提升了描述符的判别能力与鲁棒性。

(3)	设计描述符语义排序损失函数，描述符得到的匹配顺序是正样本，相同语义次正样本，不同语义负样本。监督描述符融合多分类语义信息并且保留局部纹理多样性。整体结构与现有特征点-描述符框架保持兼容，可直接嵌入视觉SLAM 系统。

\section{网络架构}
如图3-1关键点检测描述符整体架构。第一行轻量可重复关键点检测。第二行可靠性检测和经过SFFM模块得到语义融合描述符。右边第二列可靠性经过邻域差分算法后与可重复性解码器结合生成最终关键点分布。

可靠性解码器与描述符解码器共享同一编码器，将输入图像$ I\ \epsilon\ \mathbb{R}^{H\times W\times3} $作为输入。通过四个模块block1-4得到共享的特征。所有模块包含两个3×3的卷积层和步长为2的最大池化层。通过编码器得到共享的$ \frac{H}{8}\times\frac{W}{8}\times128 $的特征表示。
\begin{figure} 
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fig31.pdf}
    \caption{关键点检测描述符整体架构}
    \label{fig:31}
\end{figure}
\section{可重复可靠关键点}
为了实现关键点在多视角和光照变化下的匹配，关键点通常选择几何纹理丰富的局部点。本文称这种可以匹配的能力为可重复性。通常的选择对象是角点。但是由于环境中的动态对象上的关键点，这些匹配的关键点会计算出错误的运动变化。静态对象上的匹配关系的可靠性高，动态对象上提取的关键点可靠性低。过去的方法通常只考虑到了关键点的局部可重复性，而没有考虑到关键点可靠性。本方法得到关键点可重复性和可靠性热力图，经过邻域差分算法去除边缘区域后，将他们相乘得到最终候选的可重复可靠的关键点分布，再经过非极大值抑制后选择前k个关键点。



\subsection{关键点可重复性}
在关键点可重复性检测部分，本文借鉴了SuperPoint\cite{rf11} 的基于网格的检测策略，并沿用XFeat\cite{rf14} 的高效设计思想。具体而言，输入的灰度图像$ \ I\epsilon\mathbb{R}^{H\times W\times1}\  $首先被划分为 8×8 的网格单元，每个单元被展开为 64 维的局部特征表示，从而得到尺寸为$  \frac{H}{8}\times\frac{W}{8}\times64 $ 的特征映射。随后，经过四个 1×1 卷积层得到 输出一个$ H_c\times W_c\times(65)$的张量，其中，$H_c=H/8，Wc=W/8 $最后一个维度 65 表示 8×8 网格内 64 个像素点和 1 个 “无关键点” 类况。对输出的65个通道进行 softmax 归一化后$P=\mathrm{Softmax}(S) $得到每一个通道的概率，丢弃第65 通道，并将其重新映射为 8×8 的平面热力图，从而生成最终的关键点可重复性响应图$ {[0,1]}^{H×W }$。关键点可重复性损失函数设计：得到每个网格的真值Y，然后与$H_c\times W_c\times(65)$的概率结果计算交叉熵损失$\mathcal{L}_{det}$：
\begin{align}
Y &\in \{0,1\}^{H_c \times W_c \times 65}, \label{eq:rep_label} \\
\mathcal{L}_{rep}
&= -\frac{1}{H_c W_c}
\sum_{h=1}^{H_c} \sum_{w=1}^{W_c} \sum_{k=1}^{65}
Y_{hwk} \log P_{hwk}. \label{eq:rep_loss}
\end{align}
与 SuperPoint 不同的是，本文没有与描述符解码器共享Encoder的编码结果，而是引入了一个专门用于关键点检测的并行分支，使网络能够聚焦于图像的低层结构特征，从而在保持高效推理速度的同时增强关键点的定位精度与可重复性。在训练阶段，本文采用 MagicPoint 的单应变换策略，使用训练集生成伪关  键点标签，从而实现无监督条件下的可重复性监督学习。
\subsection{关键点可靠性}
传统的语义分割网络通常需要大量参数与多尺度特征融合，以实现像素级多类别预测。然而在关键点检测任务中，本文并不需要区分所有语义类别，只需判断关键点是否属于静态背景即可。因此，本文将语义分割的多分类任务简化为可靠/不可靠的二分类任务，将行人、车辆、自行车等动态目标视为不可靠动态类，而将道路、建筑、地面等静态背景视为可靠静态类。
在实现上，本文设计了一个轻量级的可靠性解码器（Reliability Decoder）。该模块与主干编码器共享低层特征，并通过跳跃连接融合底层轮廓信息，以增强空间分辨率和边界细节。解码器输出一个尺寸为$ H\times W\times2$的二分类概率图，表示每个像素的可靠概率。经过 softmax 归一化后，本文取可靠通道的概率，得到最终的$\ {{0,1}}^{H\times W} $可靠性热力图，其中值为 1 表示该位置的关键点可靠，值为0则不可靠。
在训练阶段，本文采用两阶段策略：预训练阶段，先使用标准语义分割数据进行多类分割训练，使解码器中高层特征具备丰富的语义区分能力。精调阶段，再将输出头改为二分类结构，进行静态/动态场景的可靠性学习，从而显著提升对动态物体的抑制能力。
为平衡前景（动态区域）与背景（静态区域）样本的不均衡，本文在损失函数中采用加权交叉熵损失（Weighted Cross Entropy Loss）其中$ w_i$为类别权重，用于增强对不可靠区域（动态物体）的惩罚：
\begin{equation}
\mathcal{L}_{\mathrm{rel}}
= -\frac{1}{N} \sum_i w_i \left[ y_i \log p_i + (1 - y_i)\log(1 - p_i) \right]
\end{equation}

\subsection{邻域差分算法}
得到可靠性热力图后，数值为1的区域是可靠区域，数值为0的区域是不可靠区域。而图像中不同对象交接处往往有明显的纹理变换，所以这类区域很容易检测出关键点，其稳定性很差。仅仅是剔除数值为0区域的关键点，仍可能保留位于对象交接边缘处的不可靠关键点。为此，引入邻域差分算法，用于检测交接边缘轮廓，并将这些区域统一标记为不可靠，从而进一步提高关键点筛选的鲁棒性。设 p 为图像中的一个像素点，$y(p)\in{0,1}$ 表示其可靠性标记，$N_3(p)$表示像素点p 的三领域。若满足
\begin{equation}
\exists\, q \in \mathcal{N}_3(p),\ \mathrm{s.t.}\ y(p) \neq y(q).
\end{equation}
则认为像素p 位于可靠区域与不可靠区域的交接边缘，其可靠性被重新定义为$y(p)=0 $。

\subsection{可靠性预测}
为了评估关键点可靠性预测的性能，本文在 Cityscapes 数据集上对二分类可靠性解码器进行了测试。评估指标包括平均交并比 (mIoU)：计算动态类和静态类预测结果与真实标签的交集/并集比例，并对两类取平均。mIoU 能直观反映整体可靠性预测精度。静态类精确率 (Precision)：静态类精确率定义为被正确预测为静态类的像素占所有预测为静态类像素的比例。由于静态区域通常占图像大部分，该指标有助于评估模型在剔除动态关键点时的误判率。静态类召回率 (Recall)衡量的是 在所有真实为静态的像素中，有多少被模型正确预测为静态，并且希望尽可能多的保留可靠关键点。


\begin{table}[htbp]
\centering
\caption{可靠性指标及结果}
\label{tab:reliability_results}
\begin{tabular}{lccc}
\toprule
模型 & mIoU & Precision\_sta & Recall\_sta \\
\midrule
Ours & 0.865 & 0.914 & 0.882 \\
\bottomrule
\end{tabular}
\end{table}

\section{语义几何融合模块SGFM}
为了显著提升局部描述符在复杂场景下的语义判别力与环境鲁棒性，本文设计了语义感知特征融合模块（Semantic-aware Feature Fusion Module）。该模块的核心思想是：利用可靠性解码器中高层提供的全局语义先验，对局部描述符进行语义锚定与特征融合。

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig32.pdf}
    \caption{语义几何融合模块SGFM}
    \label{fig32}
\end{figure}
\subsection{多模态特征对齐与几何嵌入}
语义解码器输出的特征映射 $S \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C_s}$ 蕴含了丰富的类别语义与区域上下文，而描述符解码器生成的 $D \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times C_d}$ 则精于表达几何纹理的不变性。对于提取出的第 $i$ 个关键点，其包含原始描述符 $d_i$ 与几何属性向量 $p_i = (x_i, y_i, c_i, \theta_i, s_i)$（涵盖坐标、置信度、旋转角与尺度）。首先，本文通过独立的多层感知机（MLP）将异构的视觉特征与几何信息映射至统一的特征空间，并进行加权融合，构建初始增强描述符 $f_i^{(0)}$：\begin{equation}f_i^{(0)} = \mathrm{MLP}{\mathrm{desc}}(d_i) + \mathrm{MLP}{\mathrm{geo}}(p_i)\end{equation}



\subsection{语义嵌入引导的交叉注意力交互}
随后，本文采用堆叠的自注意力模块对关键点进行全局交互。不同于传统的自注意力机制，本文利用可靠解码器中高层提取的语义特征 $s_i$ 作为 查询（Query），以引导局部描述符（作为 Key 和 Value）进行特征聚合。对于第 $n+1$ 层中第 $i$ 个关键点，其消息传递过程定义为：

\begin{equation}m_i = (s_i W_q) \odot \sum_{j \in \mathcal{P}} \left[ \mathrm{Softmax}_j \left( \frac{(s_i W_q)(f_j^{(n)} W_k)^\top}{\sqrt{d}} \right) (f_j^{(n)} W_v) \right]\end{equation}

在此公式中，$\odot$ 进一步体现了语义引导的消息调制：它确保聚合后的局部几何信息必须与当前的全局语义上下文保持一致。最终，通过残差结构与 MLP 完成特征状态的迭代更新：

\begin{equation}f_i^{(n+1)} = \mathrm{MLP}_{\text{update}} \left( [f_i^{(n)} \ ; \ m_i] \right)\end{equation}
其中，$W_q,W_k,W_v $分别表示查询、键和值的线性映射层，P 为所有关键点的集合。通过注意力堆叠，网络能够实现关键点间语义与几何信息的全局关联与特征强化，最终输出具有强语义判别力与高几何稳定性的语义感知描述符。










\section{语义感知排序损失}
传统的描述符学习通常采用成对（pairwise）或三元组\cite{rf29,rf30}（triplet）损失来进行度量学习，通过拉近匹配描述符、推远不匹配描述符来优化区分性。然而，这类局部优化方式仅关注少量样本对，无法充分反映全局排序指标\cite{rf28}（如平均准确率 AP）的优化目标，也难以有效建模语义一致性。

为此，本文引入一种语义排序损失（Semantic-aware Ranking Loss），在全局排序的框架下同时考虑几何匹配与语义一致性。

具体而言，对于一幅图像 I 的描述符集合$Q = \{ q_i \}_{i=1}^{N}$，本文以每个描述符 $q_i$为查询，在另一幅图像 $I'$的描述符集合中计算相似度排序。与传统方法仅区分“匹配/不匹配”不同，本文进一步将正样本细分为两类：
(1)几何正样本：与$ q_i$对应关键点的不同视角匹配描述符（几何匹配点），并且拥有相同语义，记为 ${P}_{\mathrm{geo}}=\{p_1, p_2, \dots, p_h\}$

(2)语义正样本：位于相同语义对象上的其他关键点描述符（同语义不同位置的点），记为 ${P}_{\mathrm{sem}}=\{p_{h+1}, \dots,p_k\}$

(3)负样本集合：不同语义对象且与查询关键点$q_i$较远处的描述符 ${P}_{\mathrm{neg}}=\{p_{k+1}, \dots, p_n\}$

本文希望模型学习到如下排序关系：
\begin{equation}
\mathrm{sim}(q_i, \mathcal{P}_{\mathrm{geo}})
>
\mathrm{sim}(q_i, \mathcal{P}_{\mathrm{sem}})
>
\mathrm{sim}(q_i, \mathcal{P}_{\mathrm{neg}}),
\end{equation}


即匹配点和同语义点的相似度应显著高于语义不同的背景点。为实现这一目标，本文采用基于平均准确率（Average Precision, AP）的可微分排序损失（Differentiable AP Loss），并在此基础上引入语义增强项，从而得到最终的语义排序损失函数：
\begin{equation}
\mathcal{L}_{\mathrm{desc}} = 1 - \mathrm{AP}_{\mathrm{sem}}(q).
\end{equation}
其中$ {\mathrm{AP}}_{\mathrm{sem}}(q)$在计算时将同语义样本视为“次级正样本”，在排名中被赋予较高权重但低于几何匹配点。该机制使模型在优化匹配准确性的同时，能够显式地感知语义相关性，提升描述符对同语义区域的聚合性和判别性。

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig33.pdf}
    \caption{语义排序损失}
    \label{fig:33}
\end{figure}
如图3-3，排序损失展示三类的排序结果。蓝色方块为查询描述符和语义相同距离靠近的正样本，蓝色三角为语义相同距离较远的次正样本，黄色圆为与查询类不同语义且距离较远的负样本。第一行为训练时候的查询结果，第二行为排序真值。最终得到有局部纹理和语义区分度的描述符。
\section{训练细节}
本文在 Cityscapes 数据集\cite{rf31}上对所提出的网络进行训练。Cityscapes 提供高分辨率的城市街景图像及对应的像素级语义标注，图像尺寸为 240×480。该数据集既包含丰富的静态场景要素（如建筑、车道、树木、天空等），又包含动态目标（如行人、车辆、自行车等），非常适合用于关键点可重复性,可靠性和描述符的学习任务。本文优化神经网络使用Adam优化器，批大小为8，学习率0.001权重衰减0.0005。关键点可重复性单独训练。关键点可靠性和描述符联合训练。描述符损失的超参数和可靠性关键点超参数分别设置为$ \lambda_{rel}=1.2, \lambda_{desc}=1.0$。最终的训练损失函数为：
\begin{align}
\mathcal{L}_1 &= \mathcal{L}_{\mathrm{rep}}, \\
\mathcal{L}_2 &= \lambda_{\mathrm{rel}} \mathcal{L}_{\mathrm{rel}}
+ \lambda_{\mathrm{desc}} \mathcal{L}_{\mathrm{desc}} .
\end{align}


\subsection{可重复性标签生成}
为了生成关键点可重复性的监督信号，本文采用 MagicPoint 的单应变换策略。具体而言，对于每幅输入图像，本文通过随机仿射与透视变换生成多组单应性配准图像，并对这些图像执行多次关键点检测。将所有变换图像中的稳定重复出现的关键点区域聚合后，生成伪标签（pseudo ground truth）作为可重复性关键点的监督信号。该过程能够有效提升无监督条件下的关键点检测性能。

\subsection{关键点可靠性学习}
在关键点可靠性学习阶段，本文首先使用 Cityscapes 的像素级语义分割标签进行多类别语义训练，以获得编码器与解码器中丰富的多类别语义特征表示。随后，本文将原始的多类别分割任务简化为动态/静态二分类任务：将行人，车辆，自行车等语义类别归为动态类，其余类别建筑、道路、树木、天空等归为静态类。通过这种转换，可靠性解码器可直接预测关键点的可靠性概率图，其中高值对应静态区域的可靠关键点，低值对应动态区域的不可靠关键点。

\subsection{语义排序损失}
描述符的训练采用基于排序优化的损失函数。对于输入的两张图像，其中一张通过单应变换（homography warping）由另一张生成，二者在公共区域中具有一一对应关系。本文以第一张图像的每个关键点描述符$ q_i$作为查询，在另一张图像中搜索对应描述符集合，定义如下样本类别：正样本对应的几何匹配关键点及其 4 像素邻域内的描述符；次正样本位于相同语义对象上的其他关键点描述符；负样本语义类别不同且欧氏距离大于 8 像素的描述符。在排序过程中，模型被优化为使匹配点与同语义关键点的相似度排名靠前，而语义不同的描述符排名靠后。通过对每个查询描述符计算平均准确率（AP）并最小化其反值，最终得到可微分的语义排序损失。该损失函数能够在优化局部特征匹配精度的同时显式引入语义一致性约束，从而提升描述符的判别性与语义聚合性。


\section{数据集与评价指标}
\subsection{比较数据集}
由于本文的设计是去除动态不可靠关键点，并且使用语义融合描述符提高辨别能力。所以本文挑选场景是户外并且包含行人车辆的数据集来测试视觉定位。本文选择使用室外场景视觉定位数据集Aschen dataset\cite{rf32}, Robotcar dataset\cite{rf32}测试本文与其他模型的指标比较。而Hpatches中大都是静态平面，所以只用来验证本文设计的语义融合描述符的匹配效果。最后在Robotcar dataset和Hpatches\cite{rf29}数据集上进行消融实验。

Aachen Day-Night 数据集包含 6,697 张白天参考图像与 1,015 张查询图像（其中 824 张白天、191 张夜晚）。该数据集具有强烈的光照变化和昼夜差异，以及室外场景中的行人，是检验特征鲁棒性的重要基准。

Oxford RobotCar数据集包含 26,121 张参考图像与 11,934 张查询图像，涵盖多个季节与不同天气条件，同时包含大量道路动态对象。这使其成为验证动态场景下关键点可靠性与跨条件描述符一致性的理想数据集。

Hpatches数据集包含116个场景696张图片。其中57个光照变化的场景，59个视角变化的场景。场景主要是静态建筑，墙面和自然风光。使用GT的单应变化和描述符指标来验证在光照，视角变化下语义融合描述符的匹配鲁棒性。




\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig34.png}
    \caption{动态场景中关键点筛选与匹配结果可视化}
    \label{fig33}
\end{figure}

\subsection{评价指标与评价方法}

视觉定位：本文采用层次化定位管线 HLoc\cite{rf33}  进行图像定位实验。HLoc\cite{rf33} 首先根据特征点匹配结果构建稀疏的三维 SfM 地图，并利用已知的相机位姿进行三角化；随后，在给定的 3D 地图中对查询图像进行定位。为了公平比较，本文将输入图像的最长边统一缩放至 1024 像素，并从所有方法中提取前 4096 个关键点进行匹配。在两组实验中，本文统一采用 HLoc 提供的评测协议。图像特征由不同方法的检测与描述模块替换，后续的 SfM 重建与位姿优化保持一致。本文报告在位置误差阈值 {0.25m, 0.5m, 5m} 与旋转误差阈值 {2°, 5°, 10°} 下，成功定位图像的比例，使用与 HLoc 相同的标准化指标。

单应估计：为了评估语义融合描述符的匹配性能，本文在 HPatches\cite{rf29} 数据集上进行了单应估计实验，所有图像均调整为 480 × 640 的分辨率，每张图像最多检测 1024 个关键点，因为要验证融合描述符的性能，所以直接使用可重复性解码器得到的关键点而不用可靠性解码器结果对动态关键点剔除。对于具有视角变化或光照变化的每对图像，本文使用最近邻匹配（Nearest Neighbor Matching）进行描述符匹配，并利用 RANSAC 计算单应矩阵。随后在 1、3、5 像素误差阈值 下评估单应矩阵估计精度，统计阈值范围内的正确匹配比例作为指标。采用 3 像素误差阈值 来计算描述符 NnmAP 与 M.Score指标，遵循该数据集的标准评测协议。

比较的方法：比较的方法包括 SuperPoint\cite{rf11}、ALIKE\cite{rf12}、SIFT\cite{rf9}、XFeat\cite{rf14}、R2D2\cite{rf13}、LiftFeat\cite{rf25} 和 SFD2\cite{rf16}。SuperPoint\cite{rf11} 是 SLAM 系统中最常用的神经网络关键点检测与描述方法，本文的关键点 GT 生成也沿用了 SuperPoint 的策略。ALIKE 引入了可微分的关键点检测模块，提高了关键点定位精度，同时保持了网络的轻量化特性。SIFT 是经典的手工设计关键点方法，历史悠久且广泛应用于各类视觉任务。SFD2 在训练过程中利用语义标签监督，使描述符获得一定的语义信息，从而提升了描述符的区分能力。XFeat 提出了轻量化的关键点检测与描述方法，计算量最低，但精度相对有限。R2D2 强调关键点可靠性，通过在图像中计算描述符相似度来剔除重复纹理或无纹理区域的不可靠关键点。LiftFeat 则将尺度不确定的深度信息转换为表面法向量，并与描述符融合，从而显著增强了描述符的判别能力

\section{实验结果}
\subsection{定性比较结果}
图\ref{fig33} 是整体结果的定性比较，在车流和行人场景下展示本方法与 SuperPoint 的特征点匹配结果。可以看出，本方法能够有效抑制行人和车辆等动态目标上的关键点，并获得更稳定、判别性更强的特征匹配。

图\ref{fig34}是单独的语义融合描述符匹配的定性比较。在 HPatches 数据集的光照变化场景下，对比了本文方法与 SuperPoint 的特征匹配结果。在相同数量的匹配描述子条件下，采用 3 像素重投影误差作为匹配判定阈值，其中绿色连线表示正确匹配，红色连线表示错误匹配。结果表明，引入 SRL 与 SFFM 的语义融合描述子能够获得更多的正确匹配，显著减少误匹配数量，说明所提出的融合描述符在区分性和鲁棒性方面优于传统 SuperPoint 描述符。
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig35.png}
    \caption{融合描述符的匹配性能对比}
    \label{fig34}
\end{figure}
\subsection{定量比较结果}
本文的方法在特征提取阶段引入了 可靠性解码器（Reliability Decoder），用于显式地抑制来自户外动态物体（如行人、车辆等）的不可靠关键点，从而提升特征匹配的稳定性与鲁棒性。此外，本文进一步将 多语义特征 融合至局部描述符中，使网络能够在 昼夜、季节及天气变化 等跨域场景下保持较强的判别能力与匹配一致性。

如表 \ref{tab:aachen_results} 所示，在 Aachen Day-Night \cite{rf32}数据集上，本文的方法在白天场景下与当前主流方法保持同一性能水平。与广泛应用于工业场景的基线算法 SuperPoint相比，在严格阈值 (0.25 m / 2°) 下的成功率由 82.6\% 提升至 85.4\%，在中等和宽松阈值下同样保持领先。本文在白天和场景中取得了更好的性能表现，而在夜晚场景下表现显著更优。这表明，融合语义与几何信息 的策略，能够在极端光照变化条件下，能够有效提升特征的区分性与稳定性，从而实现更高精度的视觉定位。

如表 \ref{tab:robotcar_results}  所示，在 RobotCar-Seasons\cite{rf32} 数据集上，本文的方法在白天与夜间场景中均取得了优异表现。在夜间这一更具挑战的跨条件场景下，本文在 (0.25 m / 2°) 阈值下的成功率达到 18.5\%，超越所有对比方法。同时在宽松阈值 (5 m / 10°) 下依然保持最高的 73.0\% 成功率。这充分验证了本文方法在 动态、光照剧烈变化的真实城市交通环境 中的稳健性与泛化能力。

如表 \ref{tab:hpatches_results}  所示，在 HPatches 的单应估计实验中，本文方法在 ε=5 和 ε=7 阈值下取得了最优性能，同时在 NN mAP 与 Matching Score 指标上均优于对比方法，表明所提出的语义融合与全局排序损失能够有效提升描述符的判别性与匹配排序能力。尽管在严格阈值 ε=3 下性能略低于部分方法，但该阈值更侧重像素级精确几何对齐，而非 SLAM 等下游任务中更为关键的匹配稳定性与整体一致性。综合来看，本文方法在更贴近实际应用需求的设置下表现出更强的鲁棒性与实用价值。

\begin{table}[htbp]
\centering
\caption{Aachen Day--Night 数据集视觉定位结果}
\label{tab:aachen_results}
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{Day} & \multicolumn{3}{c}{Night} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Method 
& 0.25m\,2$^\circ$ 
& 0.5m\,5$^\circ$ 
& 5m\,10$^\circ$ 
& 0.25m\,2$^\circ$ 
& 0.5m\,5$^\circ$ 
& 5m\,10$^\circ$ \\
\midrule
SuperPoint\cite{rf11} & 82.6 & 90.3 & 97.0 & 77.6 & 85.7 & 95.5 \\
ALIKE\cite{rf12}     & 85.7 & \textbf{92.4} & 97.0 & 81.6 & 85.7 & 99.0 \\
SFD2\cite{rf16}       & 84.2 & 92.0 & 96.7 & 77.8 & 90.9 & 98.3 \\
XFeat\cite{rf14}      & 84.7 & 90.5 & 96.5 & 77.6 & 89.8 & 98.0 \\
LifeFeat\cite{rf25}   & \textbf{86.6} & 90.1 & 97.1 & 82.1 & 89.9 & 99.1 \\
R2D2\cite{rf13}       & 85.3 & 86.7 & 92.2 & 74.8 & 80.5 & 97.8 \\
Ours      & 85.4 & 91.0 & \textbf{97.2} & \textbf{83.6} & \textbf{91.7} &\textbf{99.3 }\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{RobotCar-Seasons 数据集视觉定位结果}
\label{tab:robotcar_results}
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{Day} & \multicolumn{3}{c}{Night} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
Method 
& 0.25m\,2$^\circ$ 
& 0.5m\,5$^\circ$ 
& 5m\,10$^\circ$ 
& 0.25m\,2$^\circ$ 
& 0.5m\,5$^\circ$ 
& 5m\,10$^\circ$ \\
\midrule
SuperPoint\cite{rf11} & 56.5 & 81.5 & 97.1 & 16.9 & 41.6 & 71.5 \\
SFD2\cite{rf16}        & 55.6 & 80.2 & 96.5 & 17.0 & 42.1 & 70.0 \\
SIFT\cite{rf9}        & \textbf{57.5} & 81.9 & \textbf{98.3} & 7.8  & 13.9 & 22.1 \\
LifeFeat\cite{rf25}    & 53.5 & 82.1 & 97.6 & 18.0 & 43.0 & 70.9 \\
R2D2\cite{rf13}        & 57.4 & 81.9 & 97.9 & 18.3 & 43.4 & 72.8 \\
Ours       & \textbf{57.5} & \textbf{82.1} & 98.0 & \textbf{18.5} & \textbf{44.3} & \textbf{73.0} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Hpatches 数据集单应性估计结果}
\label{tab:hpatches_results}
\begin{tabular}{lcccccc}
\toprule
 & \multicolumn{3}{c}{Homography Estimation} & \multicolumn{2}{c}{Descriptor Metrics ($\epsilon=3$)} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6}
Method 
& $\epsilon=3$ & $\epsilon=5$ & $\epsilon=7$ 
& NN mAP & M.score \\
\midrule
SuperPoint & 68.4 & 82.9 & 88.6 & 82.1 & 47.0 \\
XFeat      & 67.2 & 83.5 & 87.8 & 81.1 & 48.7 \\
LifeFeat   & \textbf{68.9} & 84.0 & 88.9 & 86.2 & 50.8 \\
Ours       & 67.3 & \textbf{84.6} & \textbf{89.0} & \textbf{86.4} & \textbf{51.0} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{消融实验}
为了验证各个模块在本文网络中的作用，本文设计了消融实验，通过逐步添加不同模块在 RobotCar-Seasons 数据集上进行评估。具体而言，Reli 表示使用关键点可靠性解码器剔除不可靠的动态关键点。SRL 表示通过语义排序损失对描述符进行语义监督学习。SGFM 表示使用交叉注意力机制将描述符的几何，局部纹理与语义信息进行融合。

在表\ref{tab:ablation_results}  特征点提取与描述消融实验中，在基础模型（未使用任何模块）下，网络在白天场景的成功率分别为 54.6\% / 80.5\% / 94.3\%，夜间场景为 16.5\% / 41.6\% / 71.4\%。逐步引入单个模块可以显著提升性能：引入可靠性二值分割（Reli）后，白天场景提升至 55.8\% / 81.3\% / 95.0\%，夜间场景提升至 17.2\% / 42.8\% / 72.0\%；仅使用语义排序损失（SRL）或 SFFM 模块也能分别获得一定提升。进一步组合两个模块（SRL+SFFM）时，白天场景成功率达到 56.8\% / 82.0\% / 97.0\%，夜间场景达到 18.2\% / 44.0\% / 72.9\%。当三个模块同时使用时，网络在白天场景取得最高成功率 57.5\% / 82.1\% / 98.0\%，夜间场景为 18.5\% / 44.3\% / 73.0\%。

该消融实验表明，可靠性解码器、语义排序损失和语义纹理融合模块在提高关键点匹配和视觉定位性能方面均发挥了重要作用，且模块间具有互补性，联合使用可获得最佳性能。

\begin{table}[htbp]
\centering
\caption{特征点提取与描述消融实验}
\label{tab:ablation_results}
\begin{tabular}{lccc}
\toprule
Reli & SRL & SGFM & Day-all / Night-all \\
\midrule
 &  &  & (2$^\circ$, 0.25m) / (5$^\circ$, 0.5m) / (10$^\circ$, 5m) \\
\midrule
$\times$ & $\times$ & $\times$ & 54.6 / 80.5 / 94.3 \quad 16.5 / 41.6 / 71.4 \\
$\checkmark$ & $\times$ & $\times$ & 55.8 / 81.3 / 95.0 \quad 17.2 / 42.8 / 72.0 \\
$\times$ & $\checkmark$ & $\times$ & 55.2 / 81.0 / 94.8 \quad 17.0 / 42.0 / 71.8 \\
$\times$ & $\times$ & $\checkmark$ & 55.0 / 80.8 / 94.7 \quad 16.8 / 41.9 / 71.7 \\
$\times$ & $\checkmark$ & $\checkmark$ & 56.8 / 82.0 / 97.0 \quad 18.2 / 44.0 / 72.9 \\
$\checkmark$ & $\checkmark$ & $\checkmark$ & 57.5 / 82.1 / 98.0 \quad 18.5 / 44.3 / 73.0 \\
\bottomrule
\end{tabular}
\end{table}




\begin{table}[htbp]
\centering
\caption{描述符消融实验结果}
\label{tab:descriptor_ablation}
\begin{tabular}{lcc}
\toprule
Descriptor Metrics ($\epsilon=3$) & NN mAP & M.score \\
\midrule
SuperPoint + Ours (SRL + SFFM) & 85.7 & 50.1 \\
SuperPoint                     & 82.1 & 47.0 \\
Xfeat + Ours (SRL + SFFM)      & 86.5 & 51.3 \\
Xfeat                          & 81.1 & 48.7 \\
\bottomrule
\end{tabular}
\end{table}


SRL+SGFM是语义排序损失和语义几何融合模块表示语义融合后的描述符，分别使用SuperPoint和Xfeat的关键点，加上本文设计的融合描述符和他们自己对比，在Hpatches数据集下验证描述符匹配精度和匹配召回能力。

在表\ref{tab:descriptor_ablation} 描述符消融实验中本文单独将本文的语义融合的特征描述符分别和SuperPoint和Xfeat的关键点整合得到关键和描述符。实验表面再关键点不变的情况下，语义融合描述符在Hpatches数据集的光照和视角变化下，描述符的匹配精度和匹配召回率有一定提升。

\subsection{运行分析}
在 i7-13700H 与 RTX 2080 Ti 上、输入分辨率 240×320 的设置下，
对 SuperPoint、XFeat 与本文方法进行推理耗时对比。结果显示，XFeat 参数量与 FLOPs 最低，
因此 CPU/GPU 运行时间均最短。SuperPoint 处于中等水平。本文方法由于引入可靠性解码器与语义融合模块，
参数量与 FLOPs 较高，CPU/GPU 耗时略增，但仍保持在可用的实时范围内。整体上，本文方法在计算量增加的同时换取了更强的动态场景鲁棒性与描述符判别性。
% ...existing code...
\begin{table}[htbp]
\centering
\caption{运行计算量对比}
\label{tab:runtime_compute_compare}
\begin{tabular}{lccc}
\toprule
Metric & SuperPoint & XFeat & Ours \\
\midrule
Params (M)         & 1.37  &0.66  &1.59  \\
FLOPs (G)          &19.65   & 1.33  &23.79  \\
Desc. Dimension    &256   &64  &256  \\
runtime/CPU (ms)   &254       & 35      &304  \\
runtime/GPU (ms)   &32       &4.9       &41  \\
\bottomrule
\end{tabular}
\end{table}
% ...existing code...

\section{本章小结}
在这个论文将不可靠关键点检测，特征提取和描述整合到一个端到端的神经网络中，并且提出了一个SGFM模块将描述符几何纹理信息和中高层全局语义信息融合到一起，提高描述符的辨别能力，最后提供描述符的语义排序损失对描述符信息进行监督，使描述符可以保持局部纹理细节的同时融合多分类的语义提升自己的辨别能力。保持原有关键点描述符输入输出，兼容主流视觉SLAM算法，并且不用像动态SLAM\cite{yu2018ds,rf26}中引入额外的语义分割网络。

最后本文的方法在大规模室外动态场景数据集结果上，特别是Aachen Day–Night和Hpatches的夜晚由于语义信息，本方法展现了极低的性能衰减，RobotCar-Seasons 中剔除动态不可靠点，使得在位姿定位精度上比较高。最后在消融实验中验证各个模块的有效性。





